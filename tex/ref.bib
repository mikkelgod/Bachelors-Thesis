@book{Goodfellow-et-al-2016,
    title= {Deep Learning},
    author= {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher= {MIT Press},
    url= {\url{http://www.deeplearningbook.org}},
    year= {2016}
}

@misc{LSTM,
    title= {Time Series Forecasting Using LSTM Networks: A Symbolic Approach}, 
    author= {Steven Elsworth and Stefan GÃ¼ttel},
    year= {2020},
    note = {arXiv:2003.05672},
    eprint= {2003.05672},
    archivePrefix= {arXiv},
    primaryClass= {cs.LG}
}

@misc{Transformer,
    title= {Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case}, 
    author= {Neo Wu and Bradley Green and Xue Ben and Shawn O'Banion},
    year= {2020},
    note = {arXiv:2001.08317},
    eprint= {2001.08317},
    archivePrefix= {arXiv},
    primaryClass= {cs.LG}
}

@misc{Attention,
    title= {Attention Is All You Need}, 
    author= {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year= {2017},
    note= {arXiv:1706.03762},
    eprint= {1706.03762},
    archivePrefix= {arXiv},
    primaryClass= {cs.CL}
}

@misc{Positionalencoding,
    title= {Transformer Architecture: The Positional Encoding},
    author = {Amirhossein Kazemnejad},
    year = {2019},
    note = {\url{https://kazemnejad.com/blog/transformer_architecture_positional_encoding/}},
    url = {https://kazemnejad.com/blog/transformer_architecture_positional_encoding/}
      

@misc{Illustratedtransformer,
    title = {The Illustrated Transformer},
    author = {Jay Alammar},
    year = {2018},
    note = {\url{http://jalammar.github.io/illustrated-transformer/}},
    url = {http://jalammar.github.io/illustrated-transformer/}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      note={arXiv:1412.6980},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{TFT,
      title={Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting}, 
      author={Bryan Lim and Sercan O. Arik and Nicolas Loeff and Tomas Pfister},
      year={2020},
      note={arXiv:1912.09363},
      eprint={1912.09363},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{HowToCodeTransformer,
    title={How to code The Transformer in Pytorch},
    author={Samuel Lynn-Evans},
    year={2018},
    note={\url{https://towardsdatascience.com/\\how-to-code-the-transformer-in-pytorch-24db27c8f9ec}},
    url = {https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec}
    }