\documentclass[main.tex]{subfiles}
\begin{document}
\section{Discussion}
Setting aside the clear differences between the two models and taking a step back to look at this whole problem in a broader perspective, there are some things that should be discussed. This section aims to provide some different views on the method of approach with handling the dataset, the different steps that were taken to make the implementation work throughout the whole work on the project, and also what possibly could be done to improve the model even further.

\subsection{Independence of the data and features}
On top of the reasons that were mentioned earlier with the absence of extensive testing of the model elements, another reason that a clear conclusion on the generalization capabilities of the transformer could not be reached, is the lack of independency of the validation and test data from the training data. Both of these come from the same signal as the training data which means that true generalization can not be reached as the model was essentially trained on the same data as it was evaluated on, despite the fact that they come from different parts of the sequence. To counteract this, the model could have been trained on this dataset and evaluated on another dataset, or have been trained on data from some number of countries and testing on another countries data.\\
Despite this though, as mentioned earlier, a choice was made to dumb down and sub-sample the dataset to simplify the problem of time series forecasting. This was of course to make it easier to compare the bare bones model by making it a uni-variate prediction problem, but also in terms of simpler visualization in general. One could argue that this is generally not the right approach, removing much of the variation in the data, but as the focus of the project was on the models and their capabilities, and not the data, this felt like the right approach.\\
Though one could argue on the other hand that this removes some of the ability to compare models, seeing as multi-variate problems often represent how the world actually works. Other than that, this could also in some cases hurt the performance of the transformer, as having more features possibly could help the model make better predictions. An example of this is what is talked about in the paper \cite{TFT}, which presents a library based on the transformer and attention architecture, specialized for time series forecasting. One of the novel technologies here is that extra features are added to the data, giving the model more information about known information from the future such as upcoming holiday dates among other things \cite{TFT}. This gives an indication on how the transformer possibly could thrive in more feature-heavy problems, something that contains more time series, more features, and more data. But using this library as another point of evaluating the performance of a transformer model is difficult, as this architecture not only utilizes both transformers and LSTM models, but also other technologies, making it even harder to pinpoint where the performance comes from.

\subsection{Teacher forcing}
Throughout working on the actual implementation of the transformer, the method for making predictions with the model went through different phases, before ending up with the final iteration. In first trying to adapt the transformer from \cite{\textit{attention}}, from semantics analysis to numerical analysis, there were difficulties in making the transformer able to make predictions that were remotely close to the actual data. One of the methods that was tried out was \textit{teacher forcing}, which is a method that consists of training the model to make single step predictions, and then compounding single step predictions to make multi-step predictions when evaluating on the test set \cite{Teacher}. To begin with, this made it possible for the transformer to make predictions that looked like the actual data, meaning that it could capture fluctuations on a daily and weekly basis, but varied widely in terms of the values, lagging steps behind or having values that were very different. However, in the same step of the implementation process where other crucial elements of the model were added, including masking, which were not added when trying teacher forcing, running the model without teacher forcing ended up working perfectly. 

\subsection{Hyperparameter tuning and experimentation}
Following up on the statement that the transformer results possibly could have been improved by further hyperparameter tuning, it should be noted that this of course is the general case when working with neural networks. There were a few reason why this was not done, despite the fact that this is general practice when wanting to find an optimal model. Firstly, as mentioned earlier, the model on which the transformer in this assignment was based on had a set of recommended parameters, such as the number of encoder/decoder layers, number of attention heads et cetera. Though this in theory should not mean that the same model with these parameters would be compatible with the time series prediction problem in this project, especially when factoring in other parameters such as learning rate and different error measures. The combination of all these parameters make up a neural network model of a decent complexity, which leads to the other reason of the missing hyperparameter tuning. Though it is not an excuse to not implement and find the best hyperparameters, the amount of time it would take to test all the different combinations of parameters was an important factor in the decision that was made. Furthermore, keeping in line with the overall purpose of the project, tuning the transformer to find the perfect model would not necessarily add to the wanted purpose of this project, which was to simply compare the different models, and not to compare the perfect instances of said models.\\
\\
Another point of critique of the approach of the experimentation of the models deals with the visualization of the results from the models. As mentioned earlier, in the case of the transformer, the loss plot showed a validation loss lower than its training loss, which generally would not make much sense. Setting aside the reason for this, to make this point even clearer, one could have compounded the loss plots from many different runs of the model. This is both the case for the transformer model, but also for the LSTM model. Showing an average plot of all these losses would of course give a more general idea of all the experiments, however again because of time constraints, and transformer loss plots that did not vary much in general, the choice was made to simply show a loss plot that seemed to represent the general behavior of the model.


\end{document}