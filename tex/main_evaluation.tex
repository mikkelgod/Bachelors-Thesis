\documentclass[main.tex]{subfiles}
\begin{document}
\section{Evaluation}

So far two different types of models and how they could be used for time series forecasting have been presented throughout in the report. Presented first was the relatively simpler LSTM, which builds on the idea of recurrency which is inherent in the nature of time series. Secondly, the transformer was presented, which is a model type that utilizes the concept of attention among other things, representing a more advanced type of model which can be augmented to make time series predictions, as the recurrent nature of time series is not inherent in the model architecture. In this section, an evaluation of the two model architectures is presented, focusing on the comparison of the results presented in the previous sections.\\
\\
The first thing that is noticed is that the LSTM model had trouble making longer multi-step predictions. This was apparent as it output what seemed like good predictions, but what one could suspect were nothing better than simply the standard values from the sequence with which it based its predictions on. This was exemplified on figure \ref{fig:lstm_preds_1} and \ref{fig:lstm_preds_2}, where the first figure showed "close" predictions, but where the second plot clearly showed great disparities between the actual values and the predictions. This was of course when the prediction length was increased from 25 days to 50, posing a greater challenge. On the other hand, running the same experiment with the transformer, predicting the same days ahead based on the same number of days, much different results were output. On figure \ref{fig:transformer_pred1} and figure \ref{fig:transformer_pred_2}, it can easily be seen that the predictions in both cases were very close to the actuals values. Much closer than those of the LSTM model.\\
This sentiment was also mirrored in both of the models' loss that showed a much lower convergence point for the transformer, coming very close to zero loss for validation, and around $10-15\%$ for LSTM and in some experiments even worse, seen from figure \ref{fig:lstm_loss_1} and figure \ref{fig:transformer_loss}. One strange thing that can be noticed however is that, generally throughout different experiments, the validation loss of the transformer seemed to be lower than the training loss, which should normally not be the case. One could suspect that this could be the cause of the validation set being only a part of the whole data set, and therefore containing only one set of seasonal fluctuation, in contrast to the training set that has around 3 years of seasonal fluctuations. And therefore by only validating based on this single year of data, the model had an easier time predicting on the validation data.\\
The last point of comparison is then the average distance between the predictions and actual values over each batch from both models. Here it was much more clear to see, that the transformer was a lot better at capturing the long term dependencies than the LSTM model and making multi-step predictions. From figure \ref{fig:avgerrorlstm}, it shows that the average distance gets greater and greater the longer the prediction sequence is, and in figure \ref{fig:avgerrortransformer} it was easy to see that the distance stayed pretty consistently low throughout, even for the longer predictions.\\
\\
When reviewing these results and speculating whether the complicated model is actually better than the simple one, there are some things that should be taken note of. Following the theory, and comparing the results from the transformer with those from the LSTM, one could be tempted to conclude that the transformer indeed is much better at time series forecasting. And furthermore, that the addition of the crucial elements such as masking, positional encoding, and last but not least, attention, is what made the difference. As without them, the transformer is not much more than a feed-forward neural network with some dropout. However, it can be difficult in practice to compare one type of neural network to another, especially when they vary so much in complexity, which is especially the case in this instance. But without testing the transformer model without its crucial elements, removing them bit by bit and trying different configurations, it is difficult to conclude whether they actually made a difference individually. Therefore, this is something to be very aware of, and is also the reason why it can not be simply concluded that the transformer with is simply the most favorable concept for time series forecasting. However, coming back to the actual results, based on what we are seeing, one could hypothesize that \textbf{something} about the transformer makes it a better model, and attention is a concept that is here to stay, especially when working on predicting time series data.

\end{document}