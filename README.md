# Bachelors-Thesis
This repository contains the bachelors thesis made by Mikkel Godtfredsen in the spring of 2021.

The project consists of the implementation in the form of IPython Notebooks, as implementation was done in Google Colab, as well as a pdf file with the report.

## Abstract
Time series forecasting is an interesting subject in the field of machine learning as it combines thechallenges of capturing the structure in long sequences with essentially ”predicting” the future. Thisproject explores thetransformer, an advanced model architecture implementing the concept ofatten-tion, and compares it to the more simpleLong Short Term Memorymodel, a form of gated recurrentneural network that should be able to learn long term dependencies.  This project focuses on evalu-ating and comparing the two models’ abilities in making multi-step predictions which is where thechallenges in time series prediction are most clear.The project is concluded with the sentiment that further experimentation of the transformer archi-tecture would be needed to pinpoint the specific contribution of the important elements of the trans-former, this includes attention, positional encoding, and masking.  However, based on the results ofthe experimentation and evaluation of the models, it becomes clear that the transformer model per-formed much better at predicting sequences of time series data than the LSTM model, and attentionin the space of time series forecasting is definitely a concept worth exploring further.
